{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "085db8e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\akash\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\akash\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\akash\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\akash\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\akash\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\akash\\anaconda3\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tokenizers in c:\\users\\akash\\anaconda3\\lib\\site-packages (0.11.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install tokenizers\n",
    "import pandas as pd\n",
    "import re\n",
    "from tokenizers import Tokenizer, models, trainers, normalizers, pre_tokenizers, processors\n",
    "\n",
    "# Step 1: Read the dataset from CSV\n",
    "df = pd.read_csv(r'C:\\Users\\akash\\Python_DL\\project_data\\Data.csv')\n",
    "text_data = df[\"Message\"].tolist()\n",
    "\n",
    "# Step 2: Preprocess the text\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)  # Remove special characters\n",
    "    return text\n",
    "\n",
    "preprocessed_text_data = [preprocess_text(text) for text in text_data]\n",
    "\n",
    "# Step 3: Prepare the training corpus\n",
    "training_corpus = preprocessed_text_data\n",
    "\n",
    "# Step 4: Train the tokenizer\n",
    "tokenizer = Tokenizer(models.WordPiece())\n",
    "\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "\n",
    "special_tokens = [\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    "trainer = trainers.WordPieceTrainer(vocab_size=25000, special_tokens=special_tokens)\n",
    "\n",
    "tokenizer.train_from_iterator(training_corpus, trainer=trainer)\n",
    "\n",
    "# Optionally, you can save the trained tokenizer for future use\n",
    "tokenizer.save(\"custom_tokenizer.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72a7f919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'this', 'is', 'a', 'test', 'exam', '##ple', 'for', 'the', 'custom', 'token', '##izer', '[UNK]', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer, models, normalizers, pre_tokenizers, processors, decoders\n",
    "import re\n",
    "\n",
    "# Load the trained tokenizer from the saved file\n",
    "tokenizer_path = r\"C:\\Users\\akash\\Python_DL\\PROJECT_DL\\custom_tokenizer.json\"\n",
    "tokenizer = Tokenizer.from_file(tokenizer_path)\n",
    "\n",
    "# If you want to further customize the tokenizer, you can set its attributes\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "tokenizer.decoder = decoders.WordPiece(prefix=\"##\")\n",
    "tokenizer.post_processor = processors.TemplateProcessing(\n",
    "    single=\"[CLS]:0 $A:0 [SEP]:0\",\n",
    "    special_tokens=[(\"[CLS]\", 0), (\"[SEP]\", 1)],\n",
    ")\n",
    "tokenizer.normalizer = normalizers.Sequence(\n",
    "    [\n",
    "        normalizers.Replace(r\"[^\\w\\s]\", \"\"),  # Replace non-alphanumeric characters with an empty string\n",
    "        normalizers.Replace(r\"\\s+\", \" \"),  # Replace multiple whitespaces with a single space\n",
    "        normalizers.Lowercase(),\n",
    "        normalizers.NFD(),\n",
    "        normalizers.StripAccents(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Now you can use the tokenizer to tokenize and encode your examples\n",
    "example_text = \"This is a test example for the custom tokenizer.\"\n",
    "encoded_example = tokenizer.encode(example_text)\n",
    "print(encoded_example.tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22341249",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
